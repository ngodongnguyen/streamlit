import streamlit as st
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse
import time

# --- Cáº¥u hÃ¬nh API Key Groq ---
groq_key = st.secrets.get("GROQ_API_KEY") or st.secrets.get("api_keys", {}).get("GROQ_API_KEY")

if not groq_key:
    st.error("âŒ KhÃ´ng tÃ¬m tháº¥y GROQ_API_KEY trong secrets.")
    st.stop()

# --- HÃ m gá»i Groq API ---
def call_groq(prompt, model="mixtral-8x7b-32768"):
    try:
        url = "https://api.groq.com/openai/v1/chat/completions"
        headers = {
            "Authorization": f"Bearer {groq_key}",
            "Content-Type": "application/json"
        }
        data = {
            "model": model,
            "messages": [
                {"role": "system", "content": "Báº¡n lÃ  má»™t trá»£ lÃ½ AI chuyÃªn trÃ­ch xuáº¥t thÃ´ng tin tá»« ná»™i dung web."},
                {"role": "user", "content": prompt}
            ],
            "temperature": 0.3,
            "max_tokens": 1024,
        }
        response = requests.post(url, headers=headers, json=data)
        response.raise_for_status()
        return response.json()["choices"][0]["message"]["content"]
    except Exception as e:
        return f"Lá»—i gá»i Groq API: {e}"

# --- HÃ m há»— trá»£ láº¥y ná»™i dung trang web ---
def get_website_content(url):
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept-Language': 'en-US,en;q=0.9',
            'Accept-Encoding': 'gzip, deflate, br',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'DNT': '1',
        }
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')

        content_parts = []
        if soup.title:
            content_parts.append(soup.title.get_text(strip=True))
        meta_description = soup.find('meta', attrs={'name': 'description'})
        if meta_description and 'content' in meta_description.attrs:
            content_parts.append(meta_description['content'])

        paragraphs = soup.find_all('p')
        headings = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])
        list_items = soup.find_all('li')

        for tag in paragraphs + headings + list_items:
            text = tag.get_text(separator=' ', strip=True)
            if text:
                content_parts.append(text)

        full_content = " ".join(content_parts)
        return full_content[:8000]
    except requests.exceptions.RequestException as e:
        st.warning(f"KhÃ´ng thá»ƒ truy cáº­p {url}: {e}")
        return None
    except Exception as e:
        st.warning(f"Lá»—i xá»­ lÃ½ ná»™i dung tá»« {url}: {e}")
        return None

# --- Giao diá»‡n Streamlit ---
st.set_page_config(page_title="ğŸ“„ MÃ´ Táº£ Dá»± Ãn", layout="wide")
st.title("ğŸ“„ MÃ´ Táº£ Dá»± Ãn Tá»« URL")
st.caption("Nháº­p danh sÃ¡ch URL Ä‘á»ƒ AI trÃ­ch xuáº¥t mÃ´ táº£ sáº£n pháº©m/dá»‹ch vá»¥ chÃ­nh vÃ  thÃ´ng tin chi tiáº¿t.")

urls_input = st.text_area("ğŸ“¥ Nháº­p danh sÃ¡ch URL (má»—i dÃ²ng 1 link):", height=150)

if st.button("ğŸš€ PhÃ¢n tÃ­ch"):
    if not urls_input.strip():
        st.warning("âš ï¸ Vui lÃ²ng nháº­p Ã­t nháº¥t 1 URL.")
    else:
        urls = [u.strip() for u in urls_input.split('\n') if u.strip()]
        results = []

        with st.spinner("ğŸ” Äang phÃ¢n tÃ­ch..."):
            results.append("Website\tNgÃ¡ch\tNÄƒm thÃ nh láº­p\tÄá»‹a chá»‰ trá»¥ sá»Ÿ chÃ­nh\tMÃ´ táº£")

            for url in urls:
                st.write(f"ğŸ”— Äang xá»­ lÃ½: {url}")
                content = get_website_content(url)

                if content:
                    prompt = f"""
                    ## ğŸ§  Bá»‘i cáº£nh cÃ´ng viá»‡c
                    Báº¡n lÃ  má»™t chuyÃªn gia nghiÃªn cá»©u thá»‹ trÆ°á»ng vÃ  triá»ƒn khai affiliate marketing thá»±c chiáº¿n, chuyÃªn chá»n lá»c cÃ¡c ngÃ¡ch cÃ³ AOV cao, tá»· lá»‡ chuyá»ƒn Ä‘á»•i tá»‘t vÃ  ROI cao. Hiá»‡n táº¡i, báº¡n Ä‘ang xÃ¢y dá»±ng má»™t há»‡ thá»‘ng theo dÃµi ngÃ¡ch tÄƒng trÆ°á»Ÿng vÃ  chá»n lá»c sáº£n pháº©m affiliate tiá»m nÄƒng.

                    ## ğŸ¯ Má»¥c tiÃªu cá»§a báº¡n
                    Báº¡n Ä‘Æ°á»£c cung cáº¥p ná»™i dung tá»« má»™t trang web. Nhiá»‡m vá»¥ cá»§a báº¡n lÃ  **tra cá»©u vÃ  trÃ­ch xuáº¥t thÃ´ng tin chi tiáº¿t vá» trang web Ä‘Ã³ theo cÃ¡c tiÃªu chÃ­ Ä‘Ã£ Ä‘á»‹nh**.

                    ## ğŸš¨ TiÃªu chuáº©n
                    - KHÃ”NG suy diá»…n, KHÃ”NG tá»± táº¡o domain theo tÃªn ngÃ¡ch.
                    - CHá»ˆ dÃ¹ng thÃ´ng tin CÃ“ TRONG TRANG WEB.
                    - Náº¿u khÃ´ng cÃ³ thÃ´ng tin: ghi "KhÃ´ng xÃ¡c Ä‘á»‹nh".

                    ## âœ… YÃªu cáº§u Äáº§u ra (1 dÃ²ng tab-separated):
                    | Website | NgÃ¡ch | NÄƒm thÃ nh láº­p | Äá»‹a chá»‰ trá»¥ sá»Ÿ chÃ­nh | MÃ´ táº£ |

                    URL ÄANG PHÃ‚N TÃCH: {url}
                    Ná»˜I DUNG TRANG WEB:
                    {content}
                    """
                    try:
                        response_text = call_groq(prompt)
                        result_line = response_text.strip().split('\n')[-1]
                        results.append(result_line)
                    except Exception as e:
                        st.warning(f"âŒ Lá»—i xá»­ lÃ½ {url}: {e}")
                        results.append(f"{url}\tKhÃ´ng xÃ¡c Ä‘á»‹nh\tKhÃ´ng xÃ¡c Ä‘á»‹nh\tKhÃ´ng xÃ¡c Ä‘á»‹nh\tKhÃ´ng xÃ¡c Ä‘á»‹nh")
                else:
                    results.append(f"{url}\tKhÃ´ng xÃ¡c Ä‘á»‹nh\tKhÃ´ng xÃ¡c Ä‘á»‹nh\tKhÃ´ng xÃ¡c Ä‘á»‹nh\tKhÃ´ng xÃ¡c Ä‘á»‹nh")

                time.sleep(1)

        st.success("âœ… HoÃ n táº¥t.")
        final_output = "\n".join(results)
        st.text_area("ğŸ“‹ Káº¿t quáº£ phÃ¢n tÃ­ch", value=final_output, height=400)
